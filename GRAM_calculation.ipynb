{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariemtouihri/GRAM-Metric/blob/main/GRAM_calculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6TSpYJDjiif"
      },
      "source": [
        "# Import libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from scipy.integrate import quad\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function"
      ],
      "metadata": {
        "id": "nAuTz7QzkwGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4OppnluhXs4"
      },
      "outputs": [],
      "source": [
        "#  To load files\n",
        "file_path_1 = 'path to true values .npy' # reference matrices\n",
        "file_path_2 = 'path to predictions .npy' # predictions or augmented matrices\n",
        "\n",
        "true_values = np.load(file_path_1)\n",
        "predictions = np.load(file_path_2)\n",
        "\n",
        "del file_path_1, file_path_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG1-gJS-6M4-"
      },
      "source": [
        "### Normalize graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "516lko84W6-X"
      },
      "outputs": [],
      "source": [
        "def normalize_graph(dataset):\n",
        "    # Iterate over each data sample in the dataset\n",
        "    for matrix in dataset:\n",
        "        # Access the x attribute (target adjacency matrix)\n",
        "        matrix = torch.from_numpy(matrix)\n",
        "\n",
        "        # Compute min and max values for normalization\n",
        "        min_val = matrix.min()\n",
        "        max_val = matrix.max()\n",
        "\n",
        "        # Normalize the matrix in-place\n",
        "        matrix.sub_(min_val).div_(max_val - min_val)\n",
        "\n",
        "        # Convert the PyTorch tensor back to a NumPy array\n",
        "        matrix = matrix.numpy()\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "      del locals()[var]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_graph(true_values)\n",
        "normalize_graph(predictions)"
      ],
      "metadata": {
        "id": "_iwfB3SnrjlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation"
      ],
      "metadata": {
        "id": "rTYE34g_SRhm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiTYj04ebbPk"
      },
      "outputs": [],
      "source": [
        "def calculate_correlations(original_values, modified_values, metric):\n",
        "  p = []\n",
        "\n",
        "  for i in range(0, len(original_values), 90):\n",
        "\n",
        "    # Calculate Pearson Correlation (can be changed according the type of correlation to be calculated)\n",
        "    p_correlation = np.corrcoef(original_values[i:i+90], modified_values[i:i+90])[0, 1]\n",
        "    p.append(p_correlation)\n",
        "\n",
        "\n",
        "  # store results in an .npy file depending on the used metric\n",
        "  p_array = np.array(p)\n",
        "  file_path = f'file path to {metric} correlations .npy'\n",
        "  np.save(file_path, p_array)\n",
        "  print(f\"Results saved to {file_path}\")\n",
        "\n",
        "\n",
        "  # Clear local variables\n",
        "  for var in list(locals().keys()):\n",
        "        del locals()[var]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "5Fi550GcTKtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closeness"
      ],
      "metadata": {
        "id": "23Takso7N9PW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_SxVyOgR0f5"
      },
      "outputs": [],
      "source": [
        "def closeness (matrices):\n",
        "\n",
        "    closeness_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      closeness_v = nx.closeness_centrality(G, distance='weight')\n",
        "\n",
        "      closeness_list = list(closeness_v.values())\n",
        "      closeness_values.extend(closeness_list)\n",
        "\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(closeness_values)\n",
        "    file_path = f'file path to closeness values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"closeness_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "    return closeness_values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "closeness_true_values = closeness(true_values)\n",
        "closeness_predictions = closeness(predictions)\n",
        "correlations = calculate_correlations(closeness_true_values, closeness_predictions, metric='closeness')\n",
        "del closeness_true_values, closeness_predictions, correlations"
      ],
      "metadata": {
        "id": "vCF1Ho-oTkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Betweenness"
      ],
      "metadata": {
        "id": "NdV1h_YJN5Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def betweenness (matrices):\n",
        "\n",
        "    betweenness_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      betweenness_v = nx.betweenness_centrality(G, weight='weight', normalized=True)\n",
        "\n",
        "\n",
        "      betweenness_list = list(betweenness_v.values())\n",
        "      betweenness_values.extend(betweenness_list)\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(betweenness_values)\n",
        "    file_path = f'file path to betweenness values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"betweenness_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return betweenness_values"
      ],
      "metadata": {
        "id": "wXHlMaLANymn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betweenness_true_values = betweenness(true_values)\n",
        "betweenness_predictions = betweenness(predictions)\n",
        "correlations = calculate_correlations(betweenness_true_values, betweenness_predictions, metric='betweenness')\n",
        "del betweenness_true_values, betweenness_predictions, correlations"
      ],
      "metadata": {
        "id": "kVyXsiN2VoMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average neighbor degree"
      ],
      "metadata": {
        "id": "ri8InjblOTL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_n_d (matrices):\n",
        "\n",
        "    avg_n_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      avg_n_v = nx.average_neighbor_degree(G, weight='weight', source= 'in+out')\n",
        "\n",
        "      avg_n_list = list(avg_n_v.values())\n",
        "      avg_n_values.extend(avg_n_list)\n",
        "\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(avg_n_values)\n",
        "    file_path = f'file path to avg_n values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"avg_n_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return avg_n_values"
      ],
      "metadata": {
        "id": "-xS6ePLROC4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_n_true_values = avg_n_d(true_values)\n",
        "avg_n_predictions = avg_n_d(predictions)\n",
        "correlations = calculate_correlations(avg_n_true_values, avg_n_predictions, metric='avg_n_d')\n",
        "del avg_n_true_values, avg_n_predictions, correlations"
      ],
      "metadata": {
        "id": "O5MklPjrVuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diversity index"
      ],
      "metadata": {
        "id": "JleP3nUSOgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrices_with_nodes_zero (matrix):\n",
        "\n",
        "  # Generate different matrices with each node set to zero\n",
        "  matrices_with_nodes_zeroed = []\n",
        "\n",
        "\n",
        "  for i in range(matrix.shape[0]):\n",
        "\n",
        "    new_matrix = matrix.copy()\n",
        "    # Create a copy of the original matrix\n",
        "    new_matrix[i, :] = 0  # Set the ith row to zero\n",
        "    new_matrix[:, i] = 0  # Set the ith column to zero\n",
        "\n",
        "    # Append the modified graph to the list\n",
        "    matrices_with_nodes_zeroed.append(new_matrix)\n",
        "    array_of_matrices = np.array(matrices_with_nodes_zeroed)\n",
        "\n",
        "\n",
        "  return array_of_matrices\n"
      ],
      "metadata": {
        "id": "h78MVl3-OpMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glmBmqDSUrK-"
      },
      "outputs": [],
      "source": [
        "def shannon_diversity_index(graph):\n",
        "    diversity_index = 0.0\n",
        "\n",
        "    for node in graph.nodes():\n",
        "        # Get the outgoing edge weights of the node\n",
        "        outgoing_weights = [data['weight'] for _, _, data in graph.out_edges(node, data=True)]\n",
        "\n",
        "        # Calculate the relative proportion of each outgoing edge weight\n",
        "        total_weight = sum(outgoing_weights)\n",
        "        proportions = [weight / total_weight for weight in outgoing_weights]\n",
        "\n",
        "        # Calculate the Shannon Diversity Index for the node\n",
        "        node_diversity = -sum(p * math.log(p) for p in proportions if p > 0)\n",
        "\n",
        "        # Add the node's diversity to the overall diversity index\n",
        "        diversity_index += node_diversity\n",
        "\n",
        "    return diversity_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wey_OilZk2p"
      },
      "outputs": [],
      "source": [
        "def diversity (matrices):\n",
        "\n",
        "    div_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "\n",
        "      new_matrices = matrices_with_nodes_zero(matrix)\n",
        "\n",
        "      for new_matrix in new_matrices:\n",
        "        new_G = nx.DiGraph(new_matrix)\n",
        "        diversity_index = shannon_diversity_index(new_G)\n",
        "        div_values.append(diversity_index)\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(div_values)\n",
        "    file_path = f'file path to diversity values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"div_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return div_values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diversity_true_values = diversity(true_values)\n",
        "diversity_predictionss = diversity(predictions)\n",
        "correlations = calculate_correlations(diversity_true_values, diversity_predictionss, metric='diversity')\n",
        "del diversity_true_values, diversity_predictionss, correlations"
      ],
      "metadata": {
        "id": "ylhqciq2V143"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigenvector\n"
      ],
      "metadata": {
        "id": "JJ0T5v9hOzRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eigen (matrices):\n",
        "\n",
        "    eigen_c_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      eigenvector_c = nx.eigenvector_centrality(G, weight='weight', max_iter=500)\n",
        "\n",
        "      eigenvector_c_values = list(eigenvector_c.values())\n",
        "      eigen_c_values.extend(eigenvector_c_values)\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(eigen_c_values)\n",
        "    file_path = f'file path to eigen values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"eigen_c_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return eigen_c_values"
      ],
      "metadata": {
        "id": "Qw3J3e1eOytA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigen_true_values = eigen(true_values)\n",
        "eigen_predictions = eigen(predictions)\n",
        "correlations = calculate_correlations(eigen_true_values, eigen_predictions, metric='eigen')\n",
        "del eigen_true_values, eigen_predictions, correlations"
      ],
      "metadata": {
        "id": "XFOq1WjCV6KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted degree"
      ],
      "metadata": {
        "id": "RJp7bodhQa4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def w_degree (matrices):\n",
        "  w_in = []\n",
        "  w_out = []\n",
        "  for i in range(len(matrices)):\n",
        "\n",
        "    matrix = matrices[i]\n",
        "    G = nx.DiGraph(matrix)\n",
        "\n",
        "    weighted_in_degree_centrality = nx.in_degree_centrality(G)\n",
        "    w_in_degree_values = list(weighted_in_degree_centrality.values())\n",
        "\n",
        "    weighted_out_degree_centrality= nx.out_degree_centrality(G)\n",
        "    w_out_degree_values = list(weighted_out_degree_centrality.values())\n",
        "\n",
        "    w_in.extend(w_in_degree_values)\n",
        "    w_out.extend(w_out_degree_values)\n",
        "\n",
        "  # do the summation of w_in and w_out\n",
        "  w = [x + y for x, y in zip(w_in, w_out)]\n",
        "\n",
        "  # save results\n",
        "  list_array = np.array(w)\n",
        "  file_path = f'file path to w_degree values .npy'\n",
        "  np.save(file_path, list_array)\n",
        "  print(f\"Values are saved\")\n",
        "\n",
        "\n",
        "  # Clear local variables\n",
        "  for var in list(locals().keys()):\n",
        "      if var not in [\"w\"]:  # Retain variables needed for return\n",
        "          del locals()[var]\n",
        "\n",
        "\n",
        "  return w"
      ],
      "metadata": {
        "id": "orAgbV5oPARe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_degree_true_values = w_degree(true_values)\n",
        "w_degree_predictions = w_degree(predictions)\n",
        "correlations = calculate_correlations(w_degree_true_values, w_degree_predictions, metric='w_degree')\n",
        "del w_degree_true_values, w_degree_predictions, correlations"
      ],
      "metadata": {
        "id": "-oG9mralV99R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Katz"
      ],
      "metadata": {
        "id": "G9aDis09RRZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def katz (matrices):\n",
        "\n",
        "    katz_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      katz_centrality = nx.katz_centrality_numpy(G, alpha=0.5, weight=\"weight\")\n",
        "\n",
        "      katz_centrality = list(katz_centrality.values())\n",
        "      katz_values.extend(katz_centrality)\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(katz_values)\n",
        "    file_path = f'file path to katz values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"katz_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return katz_values"
      ],
      "metadata": {
        "id": "RW1ACXltRQQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "katz_true_values = katz(true_values)\n",
        "katz_predictions = katz(predictions)\n",
        "correlations = calculate_correlations(katz_true_values, katz_predictions, metric='katz')\n",
        "del katz_true_values, katz_predictions, correlations"
      ],
      "metadata": {
        "id": "Be6j0rvTWAar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hub-authority"
      ],
      "metadata": {
        "id": "r6nSo6nxRahY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hub_auth (matrices):\n",
        "\n",
        "    hub_auth_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "\n",
        "      # Calculate Hub and Authority Centrality\n",
        "      hub_scores, authority_scores = nx.hits(G, max_iter=100, tol=1e-6, normalized=True)\n",
        "\n",
        "      # Calculate combined centrality scores by summing Hub and Authority values\n",
        "      combined_centrality = {node: hub_scores[node] + authority_scores[node] for node in G.nodes()}\n",
        "\n",
        "\n",
        "      combined_centrality = list(combined_centrality.values())\n",
        "      hub_auth_values.extend(combined_centrality)\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(hub_auth_values)\n",
        "    file_path = f'file path to hub_auth values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"hub_auth_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return hub_auth_values"
      ],
      "metadata": {
        "id": "H3zdl9ERRZhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hub_auth_true_values = hub_auth(true_values)\n",
        "hub_auth_predictions = hub_auth(predictions)\n",
        "correlations = calculate_correlations(hub_auth_true_values, hub_auth_predictions, metric='hub_auth')\n",
        "del hub_auth_true_values, hub_auth_predictions, correlations"
      ],
      "metadata": {
        "id": "D8qZJwvLWCqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Harmony"
      ],
      "metadata": {
        "id": "aa3TAd2ORw66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def harmony (matrices):\n",
        "\n",
        "    harmony_values = []\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      harmony_v = nx.harmonic_centrality(G, distance=\"edge\")\n",
        "\n",
        "      harmony_list = list(harmony_v.values())\n",
        "      harmony_values.extend(harmony_list)\n",
        "\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(harmony_values)\n",
        "    file_path = f'file path to harmony values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"harmony_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return harmony_values"
      ],
      "metadata": {
        "id": "4e-4MuF-Rtrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "harmony_true_values = harmony(true_values)\n",
        "harmony_predictions = harmony(predictions)\n",
        "correlations = calculate_correlations(harmony_true_values, harmony_predictions, metric='harmony')\n",
        "del harmony_true_values, harmony_predictions, correlations"
      ],
      "metadata": {
        "id": "JZI67aThWE4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pagerank"
      ],
      "metadata": {
        "id": "AtnUePlvSBw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pagerank (matrices):\n",
        "\n",
        "    pagerank_c_values = []\n",
        "\n",
        "    for i in range(len(matrices)):\n",
        "      matrix = matrices[i]\n",
        "      G = nx.DiGraph(matrix)\n",
        "      pagerank_centrality = nx.pagerank(G, weight='weight')\n",
        "      pagerank_values = list(pagerank_centrality.values())\n",
        "      pagerank_c_values.extend(pagerank_values)\n",
        "\n",
        "\n",
        "    # save results\n",
        "    list_array = np.array(pagerank_c_values)\n",
        "    file_path = f'file path to pagerank values .npy'\n",
        "    np.save(file_path, list_array)\n",
        "    print(f\"Values are saved\")\n",
        "\n",
        "\n",
        "    # Clear local variables\n",
        "    for var in list(locals().keys()):\n",
        "        if var not in [\"pagerank_c_values\"]:  # Retain variables needed for return\n",
        "            del locals()[var]\n",
        "\n",
        "\n",
        "    return pagerank_c_values"
      ],
      "metadata": {
        "id": "qJhaGTRUSBR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagerank_true_values = pagerank(true_values)\n",
        "pagerank_predictions = pagerank(predictions)\n",
        "correlations = calculate_correlations(pagerank_true_values, pagerank_predictions, metric='pagerank')\n",
        "del pagerank_true_values, pagerank_predictions, correlations"
      ],
      "metadata": {
        "id": "yBt-2XHOWG_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create A_vectors\n"
      ],
      "metadata": {
        "id": "Tq3gnQQtahUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHbeQSqa9-4z"
      },
      "outputs": [],
      "source": [
        "# read correlations from stored files\n",
        "metrics = [\"betweenness\", \"closeness\", \"w_degree\", \"eigen\", \"pagerank\", \"katz\", \"hub_auth\", \"harmony\", \"avg_n_d\", \"diversity\"]\n",
        "all_corr_dict = {}\n",
        "\n",
        "for metric in metrics:\n",
        "  all_corr_dict[metric] = []\n",
        "\n",
        "\n",
        "A_vectors = [np.zeros((1, 10)) for _ in range(len(all_corr_dict[\"betweenness\"]))] # to intiate the length of the A_vectors\n",
        "\n",
        "for metric in metrics:\n",
        "\n",
        "  file_path = f'file path to {metric} correlations .npy'\n",
        "  data = np.load(file_path)\n",
        "\n",
        "  all_corr_dict[metric].extend(list(data))\n",
        "\n",
        "\n",
        "\n",
        "column = 0\n",
        "for metric, values in all_corr_dict.items():\n",
        "\n",
        "  for i in range(len(values)):\n",
        "    A_vectors[i][0, column] = values[i]\n",
        "\n",
        "\n",
        "  column +=1\n",
        "\n",
        "\n",
        "# save results\n",
        "file_path = f'file path to A_vectors .npy'\n",
        "np.save(file_path, A_vectors)\n",
        "print(f\"A_vectors are saved\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To delete unneeded variables to avoid memory crash\n",
        "def clear_variables():\n",
        "    # Delete variables while keeping methods and imported libraries\n",
        "    for name in list(globals().keys()):\n",
        "        if not name.startswith(\"_\") and not callable(globals()[name]) and not isinstance(globals()[name], type(__builtins__)):\n",
        "            del globals()[name]\n",
        "\n",
        "# Call the function to clear variables\n",
        "clear_variables()"
      ],
      "metadata": {
        "id": "O6EL5tLaLzCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAM calculation"
      ],
      "metadata": {
        "id": "1P6c9PgFmbFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_testing(predictions):\n",
        "\n",
        "    # Convert predictions to tensors if not already\n",
        "    predictions_tensors = [torch.tensor(matrix, dtype=torch.float32) for matrix in predictions]\n",
        "\n",
        "    product_list = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    for A in predictions_tensors:\n",
        "        # Move each tensor to the appropriate device\n",
        "        A = A.to(device)\n",
        "\n",
        "\n",
        "\n",
        "        B_tensor = torch.tensor([[0.230],\n",
        "       [0.473],\n",
        "       [-0.087],\n",
        "       [-0.067],\n",
        "       [0.133],\n",
        "       [0.022],\n",
        "       [0.001],\n",
        "       [0.022],\n",
        "       [0.309],\n",
        "       [0.260]], dtype=torch.float32)  # the outputed result from the training (of pearson correlations) according to GRAM paper\n",
        "        B_tensor = B_tensor.to(device)\n",
        "\n",
        "\n",
        "        product = torch.matmul(A, B_tensor)\n",
        "        # Move the product back to CPU and append to the list as a scalar\n",
        "        product_list.append(product.cpu().numpy())\n",
        "\n",
        "    # Convert the product list to a numpy array for mean calculation\n",
        "    product_tensor = torch.tensor(product_list)\n",
        "    mean_product = torch.mean(product_tensor)\n",
        "    std_product = torch.std(product_tensor)\n",
        "\n",
        "    print(f\"The distortion level is: {mean_product.item() * 100:.2f}%\")  # Display as a percentage with 2 decimal places\n",
        "    print(f\"The standard deviation is: {std_product.item()}\")\n",
        "\n",
        "    return mean_product.item(), std_product.item()\n"
      ],
      "metadata": {
        "id": "eP1-rOYJCNWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = f'file path to A_vectors .npy'\n",
        "A_vectors = np.load(file_path)\n",
        "gram_value = gram_testing(A_vectors)"
      ],
      "metadata": {
        "id": "s-WXN7rTTUPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAM Training"
      ],
      "metadata": {
        "id": "Enyk-CrsaG4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Need to generate graphs with augmented noise to create A_matrices instead of A_vectors where each row of the A_matrices represent the noise level (as explained in GRAM paper)"
      ],
      "metadata": {
        "id": "-QttGrrznm6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zygs7ygr-zc-"
      },
      "outputs": [],
      "source": [
        "def read_correlations():\n",
        "\n",
        "  X = {}\n",
        "\n",
        "  types = [\"pearson\"]  # other correlations can be used\n",
        "  for corr_type in types:\n",
        "    file_path = f'path to A_matrices .npy' # Path to A_matrices or A_vectors\n",
        "    A_matrices = np.load(file_path)\n",
        "    X[corr_type] = {}\n",
        "    X[corr_type][\"X_train\"] = []\n",
        "    X[corr_type][\"X_test\"] = []\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test = train_test_split(A_matrices, test_size=0.2, random_state=42)\n",
        "\n",
        "    X[corr_type][\"X_train\"] = X_train\n",
        "    X[corr_type][\"X_test\"] = X_test\n",
        "\n",
        "  return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gram_training(correlation, X, target_area1, target_area2, model):\n",
        "\n",
        "  A_tensors = [torch.tensor(A, dtype=torch.float32) for A in X[correlation][\"X_train\"]]\n",
        "\n",
        "\n",
        "  model = SurfacePredictionModel()\n",
        "  optimizer = Adam(list(model.parameters()),lr=0.01)\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 250\n",
        "  for epoch in range(num_epochs):\n",
        "      optimizer.zero_grad()\n",
        "      loss = custom_loss(A_tensors, target_area1, target_area2, model)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch % 25 == 0:\n",
        "          print(f'Epoch {epoch}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "  # Save the trained model\n",
        "  model_path = f'path to save the model after training' # Path to save the model\n",
        "  torch.save(model.state_dict(), model_path)\n",
        "  print(f'Model saved to {model_path}')\n",
        "\n",
        "\n",
        "  # The B matrix after training\n",
        "  optimized_B = model.linear.weight.detach().numpy()\n",
        "  optimized_B = optimized_B.reshape(-1,1)\n",
        "\n",
        "  return optimized_B\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference vector\n",
        "C_vector = np.array([0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0])\n",
        "\n",
        "noise1 = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "noise2 = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
        "\n",
        "noise_tensor1 = torch.tensor(noise1, dtype=torch.float32)\n",
        "noise_tensor2 = torch.tensor(noise2, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "KJWDN-MGGqL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurfacePredictionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SurfacePredictionModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 1, bias=False)  # B matrix as a linear layer without bias\n",
        "        self.linear.weight.requires_grad = True\n",
        "\n",
        "    def forward(self, A):\n",
        "        # The forward pass will multiply A with the learned B matrix\n",
        "        B = self.linear.weight\n",
        "        return torch.matmul(A, B.t())\n",
        "\n",
        "\n",
        "def fit_polynomial(x, y, degree):\n",
        "    # Create Vandermonde matrix for polynomial fit\n",
        "    powers = torch.arange(degree + 1).unsqueeze(0).repeat(len(x), 1)\n",
        "    X = torch.pow(x.unsqueeze(1), powers)\n",
        "    # Solve the least squares problem X * coeffs = y to find coeffs\n",
        "    result = torch.linalg.lstsq(X, y.unsqueeze(1))\n",
        "    coeffs = result.solution[:degree + 1, 0]  # Get the coefficients up to the required degree\n",
        "    return coeffs\n",
        "\n",
        "def approximate_integral(poly_coeffs, x_min, x_max, num_points=1000):\n",
        "    x = torch.linspace(x_min, x_max, num_points)\n",
        "    powers = torch.arange(len(poly_coeffs)).flip(0).unsqueeze(0).repeat(len(x), 1)\n",
        "    poly_values = torch.pow(x.unsqueeze(1), powers) * poly_coeffs.flip(0).unsqueeze(0)\n",
        "    integral = poly_values.sum(dim=1).mean() * (x_max - x_min)\n",
        "    return integral\n",
        "\n",
        "def custom_loss(A_tensors, target_area1,target_area2, model):\n",
        "    total_loss1 = 0.0\n",
        "    total_loss2 = 0.0\n",
        "    degree = 9\n",
        "\n",
        "    for A in A_tensors:\n",
        "        predicted_C = model(A)\n",
        "        predicted_C1 = predicted_C[:5].view(-1)  # First 5 elements, reshaped for fitting\n",
        "        predicted_C2 = predicted_C[-6:].view(-1)  # Last 6 elements, reshaped for fitting\n",
        "\n",
        "        # Fit and integrate for the first part\n",
        "        coefficients1 = fit_polynomial(noise_tensor1, predicted_C1, degree)\n",
        "        integral_value1 = approximate_integral(coefficients1, noise_tensor1.min(), noise_tensor1.max())\n",
        "        integral_value_tensor1 = integral_value1.requires_grad_(True)\n",
        "        loss1 = nn.functional.l1_loss(integral_value_tensor1, target_area1)\n",
        "        total_loss1 += loss1\n",
        "\n",
        "        # Fit and integrate for the second part\n",
        "        coefficients2 = fit_polynomial(noise_tensor2, predicted_C2, degree)\n",
        "        integral_value2 = approximate_integral(coefficients2, noise_tensor2.min(), noise_tensor2.max())\n",
        "        integral_value_tensor2 = integral_value2.requires_grad_(True)\n",
        "        loss2 = nn.functional.l1_loss(integral_value_tensor2, target_area2)\n",
        "        total_loss2 += loss2\n",
        "\n",
        "    avg_loss1 = total_loss1 / len(A_tensors)\n",
        "    avg_loss2 = total_loss2 / len(A_tensors)\n",
        "    return (avg_loss1 + avg_loss2) / 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "target_area1 = torch.tensor(0.28, dtype=torch.float32)\n",
        "target_area2 = torch.tensor(0.125, dtype=torch.float32)\n",
        "model = SurfacePredictionModel()\n",
        "optimizer = Adam(list(model.parameters()),lr=0.01)\n"
      ],
      "metadata": {
        "id": "YSRmeljXLxCB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}